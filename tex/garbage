      \path[->]<1-> node[format] (tex) {.tex file};
    \path[->]<2-> node[format, right of=tex] (dvi) {.dvi file}
                  (tex) edge node {\TeX} (dvi);
    \path[->]<3-> node[format, right of=dvi] (ps) {.ps file}
                  node[medium, below of=dvi] (screen) {screen}
                  (dvi) edge node {dvips} (ps)
                        edge node[swap] {xdvi} (screen);
    \path[->]<4-> node[format, right of=ps] (pdf) {.pdf file}
                  node[medium, below of=ps] (print) {printer}
                  (ps) edge node {ps2pdf} (pdf)
                       edge node[swap] {gs} (screen)
                       edge (print);
    \path[->]<5-> (pdf) edge (screen)
                        edge (print);
    \path[->, draw]<6-> (tex) -- +(0,1) -| node[near start] {pdf\TeX} (pdf);


\begin{itemize}
\item Purchase action
\item Click action
\item The user might click on a like button for evrey item
\end{itemize}

takes several types of interactions (e.g. clicks, purchases, tags). We will explain the metric with the interaction "like". The metrix

Solr is able to parse text streams. It extract the structure and make it searchable.

Recommenders answer the question "What are the best recommendations for a user?".
possibility to evaluate a recommender is to calculate the difference between the estimated preference and the actual preference.


In order to calculate precision and recall the implementation determines the top \verb|n| preferences for each user. It removes those preferences from the data model. It evaluates precision and recall with the new data model. It calculates a top-N recommendation list for each user and compares it with the real top-N preferences.


Another approach to evaluate a recommender is to take a broader view of the recommender problem. It's not strictly necessary to estimate preference values in order to produce recommendations. In many cases presenting a ordered list of recommendations is sufficient. The list is ordered from best to worst recommendation.


\subsection{Example data set}
\label{sec:exampledataset}

The example data set is a small set of user preferences. It has constructect properties:
\begin{itemize}
\item Items 108, 109 und 111 are similiar.
\item User 9 likes the items 111, 109 

\end{itemize}


\subsection{Movielens}
\label{sec:movielens}

The MovieLens dataset describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100023 ratings and 2488 tag applications across 8570 movies. These data were created by 706 users 

For recommenders which predict the preferences of a user we can evaluate it by calculating the difference between the estimated preference and the actual preference.
For this reason we measure the quality of the recommender with precision and recall.

Those ratings are unknown at the time of the recommendation. 
We can simulate the prefencences of the future by setting aside a small part of the real data set as test data. We split the collected input data into two sets.
\begin{itemize}
\item Training data set
\item Test data set
\end{itemize}