      \path[->]<1-> node[format] (tex) {.tex file};
    \path[->]<2-> node[format, right of=tex] (dvi) {.dvi file}
                  (tex) edge node {\TeX} (dvi);
    \path[->]<3-> node[format, right of=dvi] (ps) {.ps file}
                  node[medium, below of=dvi] (screen) {screen}
                  (dvi) edge node {dvips} (ps)
                        edge node[swap] {xdvi} (screen);
    \path[->]<4-> node[format, right of=ps] (pdf) {.pdf file}
                  node[medium, below of=ps] (print) {printer}
                  (ps) edge node {ps2pdf} (pdf)
                       edge node[swap] {gs} (screen)
                       edge (print);
    \path[->]<5-> (pdf) edge (screen)
                        edge (print);
    \path[->, draw]<6-> (tex) -- +(0,1) -| node[near start] {pdf\TeX} (pdf);


\begin{itemize}
\item Purchase action
\item Click action
\item The user might click on a like button for evrey item
\end{itemize}

takes several types of interactions (e.g. clicks, purchases, tags). We will explain the metric with the interaction "like". The metrix

Solr is able to parse text streams. It extract the structure and make it searchable.

Recommenders answer the question "What are the best recommendations for a user?".
possibility to evaluate a recommender is to calculate the difference between the estimated preference and the actual preference.


In order to calculate precision and recall the implementation determines the top \verb|n| preferences for each user. It removes those preferences from the data model. It evaluates precision and recall with the new data model. It calculates a top-N recommendation list for each user and compares it with the real top-N preferences.


Another approach to evaluate a recommender is to take a broader view of the recommender problem. It's not strictly necessary to estimate preference values in order to produce recommendations. In many cases presenting a ordered list of recommendations is sufficient. The list is ordered from best to worst recommendation.


\subsection{Example data set}
\label{sec:exampledataset}

The example data set is a small set of user preferences. It has constructect properties:
\begin{itemize}
\item Items 108, 109 und 111 are similiar.
\item User 9 likes the items 111, 109 

\end{itemize}


\subsection{Movielens}
\label{sec:movielens}

The MovieLens dataset describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100023 ratings and 2488 tag applications across 8570 movies. These data were created by 706 users 

For recommenders which predict the preferences of a user we can evaluate it by calculating the difference between the estimated preference and the actual preference.
For this reason we measure the quality of the recommender with precision and recall.

Those ratings are unknown at the time of the recommendation. 
We can simulate the prefencences of the future by setting aside a small part of the real data set as test data. We split the collected input data into two sets.
\begin{itemize}
\item Training data set
\item Test data set
\end{itemize}


We determine the top the top $N$ preferences for each user. Then we remove these values from the input data set. The resulting set is the training data set. We use this set to train the recommender engine. 

The removed entries form the test data set. Items in the test data set are the relevant items.


Precision and recall has the following parameters
\begin{itemize}
\item Size of the recommendation list $N$. How many recommended items are expected. The number of items retrieved. True positives and false positives.
\item What are relevant items. Relevant items could be a threshhold in preference. Or it could be a function that return user's average preference value plus one standard deviation.
\end{itemize}


What is the impact of the parameter relevanceThreshold. According to the class description the relevant items are the users top n preferences. 

According to the description of the parameter relevanceThreshold the items whose preference is at least the threshhold are relevant. 

@article{ekstrand11,
  title =	"Collaborative Filtering Recommender Systems",
  author =	"Michael D. Ekstrand and John Riedl and Joseph A.
		 Konstan",
  journal =	"Foundations and Trends in Human-Computer Interaction",
  year = 	2011,
  number =	2,
  volume =	4,
  bibdate =	"2011-07-07",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/fthci/fthci4.html#EkstrandRK11",
  pages =	"175--243",
  URL =  	"http://dx.doi.org/10.1561/1100000009",
}
\
  In order to minimize the memory footprint Mahout provides it's own implementation of a map.has and the key is a long primitive instead of an object. This saves memory



\subsection{Baseline Algorithm}
\label{sec:baselinealgorithm}

We compare the co-occurence based recommender to an item-based recommender. 

\subsection{Results}
\label{sec:results}
\begin{figure}
  \centering
\begin{tikzpicture}
\begin{axis}[
ybar,
%enlargelimits=0.45,
legend style={at={(0.5,-0.15)},
anchor=north,legend columns=-1},
ylabel={Recall/Precision},
symbolic x coords={Precision,Recall},
xtick=data,
%ybar=5pt,% configures ‘bar shift’
%bar width=9pt,
nodes near coords,
nodes near coords align={vertical},
]
\addplot coordinates {(Precision,0.0028) (Recall,0.00315)}; 
\addplot coordinates {(Precision,0.0175) (Recall,0.0079)};

\legend{itembased,co-coccurence with preferences}
\end{axis}
\end{tikzpicture} 
  \caption{Precision and recall comparison of an item-itembased recommonder and the cooccurrence based. The result setsize is 10}
  \label{fig:results}
\end{figure}


This article describes a recommender engine based on collaborative filtering. The recommendations are only based on user input. The recommender engine is designed to mixe any number of user actions (clicks, purchases, likes, tags). The ratings of a user and the applied tags will be used to compute recommendations.


There are serveral ways to design and build a recommender eninge

\begin{itemize}
\item Design a custom recommender engine. That approach 
\item 
\item Use the service of a high-end machine-learning consultancy. 
\end{itemize}

 This approach is proposto solve the recommender problem described in \cite{Dunning14}.

For example if the user purchased the movies ``Terminator'' and ``Transformers'' w

\begin{equation}

r_u = M h_u = 
\begin{matrix}
  1  & 0.40 & 0.9 & 0.1 \\
2 & 0.40 &1  & 0.9 & 0.1 \\
 3& 0.9 & 0.9 &1  & 0.63 \\
 4 & 0.1 & 0.1 & 0.63 &1 \\
\end{matrix}

\begin{matrix}
 1 \\
 1 \\
 0 \\
 0 \\
\end{matrix}
=
\begin{matrix}
 1.4 \\
 1.4 \\
 1.8 \\
 0.2 \\
\end{matrix}
\end{equation}

usersdescribes the circumstance that two items are similar when the same
 The similiarity we use is based on the number of users (or tags) in common between two items \cite{montgomery}.

  Calculation the the log-likelihood similarity is computationally expensive.
  The job will output the standard text version of a distributed row matrix.


  will use to connect to   Once we have deployed Apache Spark the \verb|spark-itemsimilarity| job can calculate the indicator matrix.

In order to compute the \gls{llr} similarity at the scale of big data Apache Mahout provides the \verb|spark-itemsimilarity| job.


Listing \ref{lst:weblog} shows part of an example log file that capture user actions.

\begin{lstlisting}[caption={User actions are stored in a web log.},label={lst:weblog}]
userId,movieId,tag,timestamp,action
23,103,980730408,,like
26,104,980731766,,like
40,1,animation,1306926135,tag
40,1,fantasy,1306926130,tag
40,47,dark,1306930201,tag
35,102,980730769,,like
\end{lstlisting}

The user expresses that he likes an item. The user interface provides a button to like a movie.

The user history does not contain explicit preference values. It only contains interactions of users and items.
 Hence we remove all items contained in the user's action history. Then sort the remaining items according to their  $r$
 