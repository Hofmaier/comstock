      \path[->]<1-> node[format] (tex) {.tex file};
    \path[->]<2-> node[format, right of=tex] (dvi) {.dvi file}
                  (tex) edge node {\TeX} (dvi);
    \path[->]<3-> node[format, right of=dvi] (ps) {.ps file}
                  node[medium, below of=dvi] (screen) {screen}
                  (dvi) edge node {dvips} (ps)
                        edge node[swap] {xdvi} (screen);
    \path[->]<4-> node[format, right of=ps] (pdf) {.pdf file}
                  node[medium, below of=ps] (print) {printer}
                  (ps) edge node {ps2pdf} (pdf)
                       edge node[swap] {gs} (screen)
                       edge (print);
    \path[->]<5-> (pdf) edge (screen)
                        edge (print);
    \path[->, draw]<6-> (tex) -- +(0,1) -| node[near start] {pdf\TeX} (pdf);


\begin{itemize}
\item Purchase action
\item Click action
\item The user might click on a like button for evrey item
\end{itemize}

takes several types of interactions (e.g. clicks, purchases, tags). We will explain the metric with the interaction "like". The metrix

Solr is able to parse text streams. It extract the structure and make it searchable.

Recommenders answer the question "What are the best recommendations for a user?".
possibility to evaluate a recommender is to calculate the difference between the estimated preference and the actual preference.


In order to calculate precision and recall the implementation determines the top \verb|n| preferences for each user. It removes those preferences from the data model. It evaluates precision and recall with the new data model. It calculates a top-N recommendation list for each user and compares it with the real top-N preferences.


Another approach to evaluate a recommender is to take a broader view of the recommender problem. It's not strictly necessary to estimate preference values in order to produce recommendations. In many cases presenting a ordered list of recommendations is sufficient. The list is ordered from best to worst recommendation.


\subsection{Example data set}
\label{sec:exampledataset}

The example data set is a small set of user preferences. It has constructect properties:
\begin{itemize}
\item Items 108, 109 und 111 are similiar.
\item User 9 likes the items 111, 109 

\end{itemize}


\subsection{Movielens}
\label{sec:movielens}

The MovieLens dataset describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100023 ratings and 2488 tag applications across 8570 movies. These data were created by 706 users 

For recommenders which predict the preferences of a user we can evaluate it by calculating the difference between the estimated preference and the actual preference.
For this reason we measure the quality of the recommender with precision and recall.

Those ratings are unknown at the time of the recommendation. 
We can simulate the prefencences of the future by setting aside a small part of the real data set as test data. We split the collected input data into two sets.
\begin{itemize}
\item Training data set
\item Test data set
\end{itemize}


We determine the top the top $N$ preferences for each user. Then we remove these values from the input data set. The resulting set is the training data set. We use this set to train the recommender engine. 

The removed entries form the test data set. Items in the test data set are the relevant items.


Precision and recall has the following parameters
\begin{itemize}
\item Size of the recommendation list $N$. How many recommended items are expected. The number of items retrieved. True positives and false positives.
\item What are relevant items. Relevant items could be a threshhold in preference. Or it could be a function that return user's average preference value plus one standard deviation.
\end{itemize}


What is the impact of the parameter relevanceThreshold. According to the class description the relevant items are the users top n preferences. 

According to the description of the parameter relevanceThreshold the items whose preference is at least the threshhold are relevant. 

@article{ekstrand11,
  title =	"Collaborative Filtering Recommender Systems",
  author =	"Michael D. Ekstrand and John Riedl and Joseph A.
		 Konstan",
  journal =	"Foundations and Trends in Human-Computer Interaction",
  year = 	2011,
  number =	2,
  volume =	4,
  bibdate =	"2011-07-07",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/fthci/fthci4.html#EkstrandRK11",
  pages =	"175--243",
  URL =  	"http://dx.doi.org/10.1561/1100000009",
}
\
  In order to minimize the memory footprint Mahout provides it's own implementation of a map.has and the key is a long primitive instead of an object. This saves memory



\subsection{Baseline Algorithm}
\label{sec:baselinealgorithm}

We compare the co-occurence based recommender to an item-based recommender. 

\subsection{Results}
\label{sec:results}
\begin{figure}
  \centering
\begin{tikzpicture}
\begin{axis}[
ybar,
%enlargelimits=0.45,
legend style={at={(0.5,-0.15)},
anchor=north,legend columns=-1},
ylabel={Recall/Precision},
symbolic x coords={Precision,Recall},
xtick=data,
%ybar=5pt,% configures ‘bar shift’
%bar width=9pt,
nodes near coords,
nodes near coords align={vertical},
]
\addplot coordinates {(Precision,0.0028) (Recall,0.00315)}; 
\addplot coordinates {(Precision,0.0175) (Recall,0.0079)};

\legend{itembased,co-coccurence with preferences}
\end{axis}
\end{tikzpicture} 
  \caption{Precision and recall comparison of an item-itembased recommonder and the cooccurrence based. The result setsize is 10}
  \label{fig:results}
\end{figure}


This article describes a recommender engine based on collaborative filtering. The recommendations are only based on user input. The recommender engine is designed to mixe any number of user actions (clicks, purchases, likes, tags). The ratings of a user and the applied tags will be used to compute recommendations.


There are serveral ways to design and build a recommender eninge

\begin{itemize}
\item Design a custom recommender engine. That approach 
\item 
\item Use the service of a high-end machine-learning consultancy. 
\end{itemize}

 This approach is proposto solve the recommender problem described in \cite{Dunning14}.

For example if the user purchased the movies ``Terminator'' and ``Transformers'' w

\begin{equation}

r_u = M h_u = 
\begin{matrix}
  1  & 0.40 & 0.9 & 0.1 \\
2 & 0.40 &1  & 0.9 & 0.1 \\
 3& 0.9 & 0.9 &1  & 0.63 \\
 4 & 0.1 & 0.1 & 0.63 &1 \\
\end{matrix}

\begin{matrix}
 1 \\
 1 \\
 0 \\
 0 \\
\end{matrix}
=
\begin{matrix}
 1.4 \\
 1.4 \\
 1.8 \\
 0.2 \\
\end{matrix}
\end{equation}

usersdescribes the circumstance that two items are similar when the same
 The similiarity we use is based on the number of users (or tags) in common between two items \cite{montgomery}.

  Calculation the the log-likelihood similarity is computationally expensive.
  The job will output the standard text version of a distributed row matrix.


  will use to connect to   Once we have deployed Apache Spark the \verb|spark-itemsimilarity| job can calculate the indicator matrix.

In order to compute the \gls{llr} similarity at the scale of big data Apache Mahout provides the \verb|spark-itemsimilarity| job.


Listing \ref{lst:weblog} shows part of an example log file that capture user actions.

\begin{lstlisting}[caption={User actions are stored in a web log.},label={lst:weblog}]
userId,movieId,tag,timestamp,action
23,103,980730408,,like
26,104,980731766,,like
40,1,animation,1306926135,tag
40,1,fantasy,1306926130,tag
40,47,dark,1306930201,tag
35,102,980730769,,like
\end{lstlisting}

The user expresses that he likes an item. The user interface provides a button to like a movie.

The user history does not contain explicit preference values. It only contains interactions of users and items.
 Hence we remove all items contained in the user's action history. Then sort the remaining items according to their  $r$


Co-occurence in the context of the recommender engine could be user-item interaction (like, purchase, etc) and tag-item associations is the number of same users that interact with them.


According to \cite{Dunning93} the log-likelihood similiarity is suitable for data that only captures the interaction and no preference values between users and items. 

In a relational database a row either matches a query or it does not. That is the reason why a search engine is more suitable for our use case.

 of a document  to the relevancy  order by a score that indicates the strength of the match of the document to the query.
  is to represent A search engine that uses the vector space model represent each document and the query as a vector.


In order to retrieve a ranked relevant documents the search engine assign a similarity score   all documents  calculates the the dot product between $q$ and $d$. The cosine is a measure of similarity between to items.
Terms with a high frequency of occurence over all documents 

Search engines assign every term in a document a weight.

A search engine ranks the documents matching a query. It computes a score for every matching document with respect to a query.

That is the reason why a search engine is more suitable for our use case.
The rows are items. The columns show which users liked which item

\begin{table}
  \centering
\begin{center}
\begin{tabular}{rrrrrr}
  & 1 & 2 & 3 & 4 & 5\\
1 & 4 & 2 & 3 & 2 & 3\\
2 & 2 & 3 & 2 & 1 & 3\\
3 & 3 & 2 & 3 & 2 & 3\\
4 & 2 & 1 & 2 & 3 & 3\\
5 & 3 & 3 & 3 & 3 & 4\\
 &  &  &  &  & \\
\end{tabular}
\end{center}
  \caption{Co-occurence matrix for item purchases}
  \label{tab:cooccurencematrix}
\end{table}
\begin{table}
  \centering
\begin{center}
\begin{tabular}{rrrrrr}
  & 1 & 2 & 3 & 4 & 5\\
1 &   & 0.40 & 0.81 & 0.63 & 0\\
2 & 0.40 &  & 0.40 & 0.63 & 0\\
3 & 0.81 & 0.40 &  & 0.63 & 0\\
4 & 0.63 & 0.63 & 0.63 &  & 0\\
5 & 0 & 0 & 0 & 0 & \\
 &  &  &  &  & \\
\end{tabular}
\end{center}
  \caption{Indicator matrix for item purchases}
  \label{tab:indicatormatrix}
\end{table}

One the right set of document are found it is important to order those documents based upon relevancy to ensure the best matches are on top of their search result.

represent the collection of document as matrix $M$. Each row of $M$ is a document vector.
If we want to compute all relevancy scores the search engine would compute the matrix vector product of $M$ and $q$.


Instead of words we index items that are simil
We view documents as sequence of terms. The search engine computes a vector
Documents can contain serveral fields. Fields contain a sequence of terms. 

 If we replace the query $q$ with the user's action history $h_u$ and the documents vectors  $\vec{v}(d)$with item \gls{llr} indicators the search engine will return a top-N recommendation list. All we have to do is remove the items allready known to the user.

A common approach to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as query.

Let's say we represent the recommendations for a user as a vector $r$. The elements of $r$ are floating point number which represent preferences for all items for a user. Most collaborative filtering type recommenders compute $r$ by multiplying the given preferences of a user $h_u$ with the indicator matrix $M$ for all items. In our example $M$ contains the similarity values of the log-likelihood cooccurence.

\begin{equation}
  \label{eq:cf}
  r = h_u M
\end{equation}

The recommender ranks the items by the score and presents them to the user. These items form the recommendations


 compute the relevancy of a document $d$ that match the query keywords,
 Equations \ref{eq:cf} actually means to compare the user history $h_u$ to the rows of the indicator matrix $M$. This result in a vector $r$ containing a score that indicates the strength of the match of the item to the history $h_p$. The recommender ranks the items by the score and presents them to the user. These items form the recommendations.


This is exactly what the ranked retrieval feature of a search eninge does.
The user history $h_U$ is the query. The items are the documents. And the text of the fields contain similar items.
This is why we deploy Solr to build a recommender.

   node[anchor=north,below]{recommendation}
   to compute the similarity of two items.
   If we store similar items similar items we can query the search engine with item-id uas a vector contaning item ID's.
   \begin{lstlisting}[caption={To add a payload to a term we have to delimit the payload with a pipe. .}]
  23|0.9
\end{lstlisting}


All movies are stored as documents in a NoSQL database (we use Apache Solr). The documents contain metainformation (e.g. title, tags, genre) about the items in fields. The fields are indexed by a search engine and made searchable.

In addidion the document has indicator fields. Indicator fields contain id's of that are found to be worth recommending in the co-occurence analysis.
Solr is used in the offline and the online part of the recommendation engine.



The items and their corresponding similarity indicators from the Apache Spark job are stored with Apache Solr. 

 In addidtion we populate a filed for every indicator with the similar item ID's discovered with the coocuccence similartiy from section \ref{sec:llr}.

In order to build a recommender using a search engine we store the output of the co-occurence analysis in Solr. The search engine actually delivers the recommendations to our users.

\subsection{Two-parts design}

The recommender described in this article is divided in two parts.
\begin{itemize}
\item Computation of simililarity and the update of the text search engine is done offline, ahead of time.
\item Recommendations are generated instantly by quering the text search engine using rescents actions of the user.
\end{itemize}
