      \path[->]<1-> node[format] (tex) {.tex file};
    \path[->]<2-> node[format, right of=tex] (dvi) {.dvi file}
                  (tex) edge node {\TeX} (dvi);
    \path[->]<3-> node[format, right of=dvi] (ps) {.ps file}
                  node[medium, below of=dvi] (screen) {screen}
                  (dvi) edge node {dvips} (ps)
                        edge node[swap] {xdvi} (screen);
    \path[->]<4-> node[format, right of=ps] (pdf) {.pdf file}
                  node[medium, below of=ps] (print) {printer}
                  (ps) edge node {ps2pdf} (pdf)
                       edge node[swap] {gs} (screen)
                       edge (print);
    \path[->]<5-> (pdf) edge (screen)
                        edge (print);
    \path[->, draw]<6-> (tex) -- +(0,1) -| node[near start] {pdf\TeX} (pdf);


\begin{itemize}
\item Purchase action
\item Click action
\item The user might click on a like button for evrey item
\end{itemize}

takes several types of interactions (e.g. clicks, purchases, tags). We will explain the metric with the interaction "like". The metrix

Solr is able to parse text streams. It extract the structure and make it searchable.

Recommenders answer the question "What are the best recommendations for a user?".
possibility to evaluate a recommender is to calculate the difference between the estimated preference and the actual preference.


In order to calculate precision and recall the implementation determines the top \verb|n| preferences for each user. It removes those preferences from the data model. It evaluates precision and recall with the new data model. It calculates a top-N recommendation list for each user and compares it with the real top-N preferences.


Another approach to evaluate a recommender is to take a broader view of the recommender problem. It's not strictly necessary to estimate preference values in order to produce recommendations. In many cases presenting a ordered list of recommendations is sufficient. The list is ordered from best to worst recommendation.


\subsection{Example data set}
\label{sec:exampledataset}

The example data set is a small set of user preferences. It has constructect properties:
\begin{itemize}
\item Items 108, 109 und 111 are similiar.
\item User 9 likes the items 111, 109 

\end{itemize}


\subsection{Movielens}
\label{sec:movielens}

The MovieLens dataset describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100023 ratings and 2488 tag applications across 8570 movies. These data were created by 706 users 

For recommenders which predict the preferences of a user we can evaluate it by calculating the difference between the estimated preference and the actual preference.
For this reason we measure the quality of the recommender with precision and recall.

Those ratings are unknown at the time of the recommendation. 
We can simulate the prefencences of the future by setting aside a small part of the real data set as test data. We split the collected input data into two sets.
\begin{itemize}
\item Training data set
\item Test data set
\end{itemize}


We determine the top the top $N$ preferences for each user. Then we remove these values from the input data set. The resulting set is the training data set. We use this set to train the recommender engine. 

The removed entries form the test data set. Items in the test data set are the relevant items.


Precision and recall has the following parameters
\begin{itemize}
\item Size of the recommendation list $N$. How many recommended items are expected. The number of items retrieved. True positives and false positives.
\item What are relevant items. Relevant items could be a threshhold in preference. Or it could be a function that return user's average preference value plus one standard deviation.
\end{itemize}


What is the impact of the parameter relevanceThreshold. According to the class description the relevant items are the users top n preferences. 

According to the description of the parameter relevanceThreshold the items whose preference is at least the threshhold are relevant. 

@article{ekstrand11,
  title =	"Collaborative Filtering Recommender Systems",
  author =	"Michael D. Ekstrand and John Riedl and Joseph A.
		 Konstan",
  journal =	"Foundations and Trends in Human-Computer Interaction",
  year = 	2011,
  number =	2,
  volume =	4,
  bibdate =	"2011-07-07",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/fthci/fthci4.html#EkstrandRK11",
  pages =	"175--243",
  URL =  	"http://dx.doi.org/10.1561/1100000009",
}
\
  In order to minimize the memory footprint Mahout provides it's own implementation of a map.has and the key is a long primitive instead of an object. This saves memory



\subsection{Baseline Algorithm}
\label{sec:baselinealgorithm}

We compare the co-occurence based recommender to an item-based recommender. 

\subsection{Results}
\label{sec:results}
\begin{figure}
  \centering
\begin{tikzpicture}
\begin{axis}[
ybar,
%enlargelimits=0.45,
legend style={at={(0.5,-0.15)},
anchor=north,legend columns=-1},
ylabel={Recall/Precision},
symbolic x coords={Precision,Recall},
xtick=data,
%ybar=5pt,% configures ‘bar shift’
%bar width=9pt,
nodes near coords,
nodes near coords align={vertical},
]
\addplot coordinates {(Precision,0.0028) (Recall,0.00315)}; 
\addplot coordinates {(Precision,0.0175) (Recall,0.0079)};

\legend{itembased,co-coccurence with preferences}
\end{axis}
\end{tikzpicture} 
  \caption{Precision and recall comparison of an item-itembased recommonder and the cooccurrence based. The result setsize is 10}
  \label{fig:results}
\end{figure}
