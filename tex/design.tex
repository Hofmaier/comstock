\section{Co-occurence based multimodal recommender}
\label{sec:design}
\begin{figure}
  \centering
  \begin{tikzpicture}
    \node (rec) at(180:2cm) {Recommender};
    \node (hist) at(60:2cm){History};
    \node (se) at(300:2cm){Search engine};
    \node (user) [left of=rec,node distance=3cm]{User};
    \draw[->, >=latex] (170:2cm) 
    arc (170:80:2cm);
  \end{tikzpicture}
  \caption{Simplified dataflow diagram}
  \label{fig:topndataflow}
\end{figure}

The recommender we discuss will use past user behavior and metadata of items to compute the similarity between all items. Hence it is a hybrid recommender.
We use \gls{coocc} to compute the similarity of two items. Because search engine are a way of finding similar items we deploy the recommender queries a search engine in order to find similar items to the one the user already exressed some interest.

The recommender discussed in this article has to parts.
\begin{description}
\item[Analysing User Input (offline)] In this part the user action are analised in order to compute the similarities of items. The similiarities are stored as indicators.
\item[Generate personalized recommendations (online)] A systems formats a list of recommended items.
\end{description}


The process of procucing a \gls{topn} can be divided into several steps. Figure \ref{fig:topndataflow} illustrates this process.

\begin{enumerate}
\item User requests a \gls{topn}.
\item The recommenders looks up items that appear in the recent user history.
\item Based on the user' action history the recommender forms a for the search engine query.
\item The search engine return ranked list of items according to the query.
\item The recommender removes item already known to the user and present him a \gls{topn}.
\end{enumerate}

All movies are stored as documents in a NoSQL database (we use Apache Solr). The documents contain metainformation (e.g. title, tags, genre) about the items in fields. The fields are indexed by a search engine and made searchable.

In addidion the document has indicator fields. Indicator fields contain id's of that are found to be worth recommending in the co-occurence analysis.

The section first describes the used input data. Than we explain the process of computing \gls{topn} and how similarity between items is measured and how the computation is implemented

\input{inputdata}
\input{topnmath}
\input{similarity}

\subsection{Using more than one type of behavior}
\label{sec:multimodal}

Most collaborative filtering algorithm use only explicit or implicit ratings to compute similarity.
But we can improve the performance of the recommender engine by using multiple types of user actions. In addition to likes we could use tag-associations to compute the similariy. In table \ref{tbl:llr} we count co-occurence of items in a user's like action history. Instead of the action history we could use tags that are associated items. We count the co-occurence of items associated with a tag.

Suppose we compute a \gls{indicatorm} based on likes $M_l$ and one based on tag associations $M_t$. An $h_l$ is a user's history of ``likes'' and $h_t$ is the user's tag history. Then we can compute the recommendations $r$ with

\begin{equation}
  \label{eq:multi}
  r = h_l M_l + h_t M_t
\end{equation}

In our demo web application we use ``likes'' and tags but virtually all user actions can be used to improve the recommendation.

\subsection{Why can we use a search engine to produce \gls{topn}?}
\label{sec:relation}

The \gls{rec} uses a search engine to produce a \gls{topn}. We can deploy a search engine in order to provide recommendations because there are similarities between the computation of top-N recommendation task and the retrieval or a ranked search result set.
 This section explains why the deployment of a search engine is suitable for the top-N recommendation task.

A search engine enables user to search a collection of documents for specified keywords in a query.It returns an sorted set of documents that match the query. The result set is sorted by relevancy. The top documents are the most relevant to the query. This process is called \gls{rankedretrieval}. It does this by calculating a similarity scorbetween each document and the query and then sorts the result by this score. The score indicates the strength of the match against the query. This is one of the main use cases where search engines shine compared to relational databases. There a row either matches a query or it does not. 

One way to calculate the similarity between a query and a document is to use the vector space model.
In the vector space model each document $d$ and the query $q$ are represented as a vectors $\vec{v}(d)$ and $\vec{v}(q)$. The vector contains an element for each term. It maps every term $t$ to a tf-idf weight. tf-idf reflects how important a term is to document in the collection (see \cite{Manning} for a detailed description). 
The similarity score between two items is equal to the dot product.
\begin{equation}
  \label{eq:score}
  \text{score}(d,q) = \vec{v}(d) \cdot \vec{v}(q)
\end{equation}

We represent the collection of document as matrix $M$. Each row of $M$ is a document vector. If we want to compute all relevancy scores the search engine would compute the matrix vector product of $M$ and $q$.
\begin{equation}
  \label{eq:ser}
  r = M q
\end{equation}

This is similar to the computation for the \gls{topn} described in section \ref{sec:problem}. If we replace the query $q$ with the user's action history $h_u$ and the documents vectors  $\vec{v}(d)$with item \gls{llr} indicators the search engine will return a top-N recommendation list. All we have to do is remove the items allready known to the user.

\todo{tabell einfuegen mit vergleich search engine fuer documents similarity engine fuer item}o

\begin{table}
\begin{center}
\begin{tabular}{ll}
 search engine & \gls{rec}\\
 document & item\\
 query & user's action history \\
term & itemid \\
\end{tabular}
\end{center}
\caption{Comparison of }
\label{tbl:llr}
\end{table}

In addition 

A common approach to users for coming up with good queries is to think of words that would likely appear in a relevant document, and to use those words as query.

Let's say we represent the recommendations for a user as a vector $r$. The elements of $r$ are floating point number which represent preferences for all items for a user. Most collaborative filtering type recommenders compute $r$ by multiplying the given preferences of a user $h_u$ with the indicator matrix $M$ for all items. In our example $M$ contains the similarity values of the log-likelihood cooccurence.

\begin{equation}
  \label{eq:cf}
  r = h_u M
\end{equation}

Equations \ref{eq:cf} actually means to compare the user history $h_u$ to the rows of the indicator matrix $M$. This result in a vector $r$ containing a score that indicates the strength of the match of the item to the history $h_p$. The recommender ranks the items by the score and presents them to the user. These items form the recommendations.

This is exactly what the ranked retrieval feature of a search eninge does.
The user history $h_U$ is the query. The items are the documents. And the text of the fields contain similar items.

In addition we can use TF-IDF \cite{Manning} weighting to mitigate popular items.

This is why we deploy Solr to build a recommender.

We store all items as documents in Solr. The documents contain the metadata like (title, genre, tags, etc). In addidtion we populate a filed for every indicator with the similar item ID's discovered with the coocuccence similartiy from section \ref{sec:llr}.

\begin{lstlisting}[caption={Item metadata and similar items are stored in Solr.},label={lst:solrdoc}]
{
    "id": "1",
    "title": ["Toy Story (1995)"],
    "tags":"Pixar animation fantasy",
    "likeindicator": "1688 1834 3893 4366 6281 33162 50872 53000 ",
    "_version_": 1505056335358591000
}
\end{lstlisting}

In order to build a recommender using a search engine we store the output of the co-occurence analysis in Solr. The search engine actually delivers the recommendations to our users.

\subsection{Retrieve recommendation}

In order to produce recommendations we compose a Solr query from the user history. The user history is stored in the web log. The web server sends this query to Solr. Solr responds with a ranked result set. The web server then formats the response from Solr and sends a list of recommended items to the user.

\begin{figure}
\centering
\begin{tikzpicture}[node distance=20mm,
data/.style={
rectangle,
draw,
thin,
minimum height=3.5em
},
to/.style={->,>=stealth',shorten >=1pt,semithick,font=\footnotesize},
]
\node (web) [data] {Web server};
\node (log) [data,below of = web, align=left] {User actions\\log file};
\node (browser) [data,left of=web,node distance=50mm] {Webbrowser};
\node (solr) [data,right of=web,node distance=50mm] {Search engine};
\draw[to] (web) -- (log);
\draw[to] (browser) -- node[midway,above] {user actions} (web);
\end{tikzpicture}
\caption{The web server sends this query to Solr. Solr responds with a ranked result set.}
\end{figure}


\subsection{Integration}
\label{sec:integration}

\verb|updatesearchengine| will index all movies in Solr.

Solr is used in the offline and the online part of the recommendation engine.

The items and their corresponding similarity indicators from the Apache Spark job are stored with Apache Solr. 

\subsection{Parameters}
\label{sec:parameters}

This section descripes the paratemter of the recommender discussed in this report.
\begin{description}
\item[similarity threshold] We have to define a threshold to separate similar items occording to the LLR similarity from the rest (e.g. 0.5).
\item[user history to consider] We retrieve recommendations with a part of the user history. We have to define the number of log entries to consider.
\end{description}

\subsection{Two-parts design}

The recommender described in this article is divided in two parts.
\begin{itemize}
\item Computation of simililarity and the update of the text search engine is done offline, ahead of time.
\item Recommendations are generated instantly by quering the text search engine using rescents actions of the user.
\end{itemize}
