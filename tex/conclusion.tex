\section{Conclusion}
\label{sec:conclusion}

We have built a prototype of a \gls{multimodal} \gls{rec} with a search engine according to the suggestions of \cite{Dunning14}. 
This approach simplifies the development process and lowers the entry barrier for developers without a strong mathematical background because we do not have to make decisions which algorithms or similarity metrics to choose. 

User interaction is the best clue users want.
The design allows to use a variety of user interactions and meta data as input data. This hybrid approach improves the performance of the \gls{topnt}. Multiple \glspl{indicator} are stored in distinct fields within Solr. The recommender engine can be extended with additional \glspl{indicator}.

The calculation of the \gls{coocc} \gls{indicatorm} and therefore the update process of the search engine is computationally expensive. Mahout's \verb|spark-item-similarity| job can compute the \gls{indicatorm} at scale.
The two part design of the \gls{rec} allows us to do the heavy lifting upfront. In return the \gls{topnt} is performed blazingly fast.

A large amount of work was required to convert formats between the user history database, the Apache Mahout and Apache Solr. 

The proposed design did not describe the inclusion of item similarity values in the search engine. We used the payload mechanism of Apache Lucene to weight indicators with corresponding similarity value.

Due to the absence of evaluation tests in \cite{Dunning14} we evaluated the recommender with the accuracy metrics \gls{precision} and \gls{recall}. The usefulness of these metrics depends on the definition of relevant items. We defined items that the user has given a high rating as relevant. The evaluation method used in this report penalizes items that the user never has rated or even seen. 

Another problem with precision and recall emerges when the \glspl{preference} are Boolean data, like in the demo application. Without a numerical rating value we can not pick the $N$ best rated items but we have to select $N$ items at random. For this reason we used the MovieLens data set that contains preference values and we simulated \glspl{useraction}.

The evaluation results of the \gls{rec} are not overwhelming. Compared to the \gls{itembased} recommender the increase of quality is small. But the evaluation method only considers items that the users already liked as relevant. We do not know how useful a recommendation for an individual user is. It could be that the \gls{rec} is suggesting items that the user has never seen and that are desirable for the user.

The system may be improved through the use of cross \gls{coocc}. The presented recommender engine maps every type of user interaction to one \gls{indicator}. For example we collect all purchase interaction and create a \gls{topn} for purchases. But we could also use other user interactions, like ``view'' in a so called cross-co-occurrence calculation, to create purchase indicators. 

