\section{Installation Guide}
This section describes the usage of the various scipts and the demo web application created during the project.
\subsection{Tools, libraries and frameworks}
The web application demo and the evaluator were developed on a Ubuntu 15.04. 
The following software was used to build the evaluator, the search engine update job and the web application. We assume that they are installed on the user's computer.
\begin{itemize}
\item JVM 1.8.45 OpenJDK. Solr, Spark and Play run on the JVM.
\item Scala Compiler 2.11.7. The web application is written in Scala.
\item sbt 0.13.8 (Scala Build Tool) . The web application can be build with sbt.
\item Maven 3.19 (Java build tool). Maven was used to build Spark and the evaluator.
\item Solr version 4.7.2
\item Spark version 1.1.1. The Mahout shell job \verb|spark-itemsimiliarity| uses Spark to compute LLR ratios.
\item Play version 2.4. We used the Play framework because it runs on the JVM and we can use the Solr Client Library in the web server.
\item AngularJS. Javascript Framework. Used for WebGUI.
\item Mahout 0.10.1. The machine learning library Apache Mahout is used to compute the LLR ratios and for the evaluation.
\item sqlite3 3.8.7.4. We use sqlite3 because there are JDBC drivers and a python interface.
\item python 2.7.9. Python is used to transform the date from sqlite3 to the input format of Apache Spark.
\end{itemize}

\subsection{Starting Apache Solr}
\label{sec:startsolr}
The demo web application and the update script depend on a running Solr instance.
To start a Solr instance with the movielens core run
\begin{verbatim}
java -jar start.jar
\end{verbatim}
in the directory \verb|solr|.
This will start a Jetty instance that is listening on port 8983. To access the administration web interface visit \verb|http://localhost:8983/solr/| with a web browser.

\subsection{Start the demo web application}

The demo web application requires a running Solr instance. Make sure you executed the steps described in section \ref{sec:startsolr} before you start the web application. The Solr instance that the web application will use can be configured in the file \verb|movierecommender/conf/application.conf|. The URL of the Solr instance can be set with the key \verb|solr.url|. For example
\begin{verbatim}
solr.url="http://localhost:8983/solr/movielens"
\end{verbatim}

To run the demo web application change to the directory \verb|movierecommender| and execute the sbt command \verb|sbt run|.

\subsection{Indicator computation- and update script}
In order to run the Python script \verb|updateSolr.py| the following environment is required.
\begin{itemize}
\item Apache Mahout shell is installed.
\item sqlite3 client is installed (can be installed with \verb|apt-get install sqlite3| on a system with Aptitude). 
\item The script requires a running spark instance. Apache Spark can be downloaded from \url{http://spark.apache.org/downloads.html}. Spark contains a shell script \verb|sbin/start-master.sh| to start a master node.
\item A running Solr instance with the movielens core. Make sure you executed the steps described in section \ref{sec:startsolr} before you start the web application.
\end{itemize}

In addition the following environment variables must be set:
\begin{description}
\item[MAHOUT\_HOME] Directory of the Mahout installation.
\item[JAVA\_HOME] JDK installation directory.
\item[SPARK\_HOME] Directory of the Spark installation.
\item[MASTER] Url of the spark master node (e.g. spark://localhost:7077).
\end{description}

The script takes two parameters. The file of the sqlite3 database and the URL of the Solr instance. For example:
\begin{verbatim}
python updateSolr.py movie.db http://localhost:8983/solr/movielens
\end{verbatim}

The shell script \verb|startUpdateSolr.sh| will run this script with an example configuration.

\subsection{Running the evaluation}

The directory \verb|evaluator| contains the evaluation program descibed in section \ref{sec:evaluation}. In order to build it change to the directory \verb|evaluator| and run the maven command
\begin{verbatim}
mvn clean compile package assembly:single
\end{verbatim}
This creates an executable jar archive. To start the evaluation run 
\begin{verbatim}
java -jar target/evaluator-0.0.1-jar-with-dependencies.jar
\end{verbatim}

The program expects the following parameters:
\begin{description}
\item[precisionat] Set the size $N$ of the top-N list.
\item[Solr instance URL] URL of the Solr instance.
\item[Ratings filepath] File path of a CSV file that contains user ratings
\item[Tags filepath] File path of a CSV file that contains tag - item interactions.
\end{description}

The shell script \verb|startevaluator.sh| starts an example evaluation.

\section{Source code listings}
\label{sec:listings}

\begin{lstlisting}[caption={To simulate the user action ``like'' we extract all ratings equal or above a score of 4.0 and use the result as training set},label={lst:pref2like}]
 public GenericBooleanPrefDataModel pref2like(DataModel dataModel,
	float threshold) {
	try {
	FastByIDMap<FastIDSet> userlikes = new FastByIDMap<FastIDSet>(
					dataModel.getNumUsers());
	LongPrimitiveIterator iter = dataModel.getUserIDs();
	while (iter.hasNext()) {
          long userid = iter.nextLong();
          PreferenceArray prefs = dataModel
		.getPreferencesFromUser(userid);
          FastIDSet ids = new FastIDSet(prefs.length());
          for (Preference p : prefs) {
		if (p.getValue() >= threshold) {
                  ids.add(p.getItemID());
			}
		}
	userlikes.put(userid, ids);
	}
    return new GenericBooleanPrefDataModel(userlikes);
\end{lstlisting}


\begin{lstlisting}[ caption={Implementation of top popular recommender}]
Map<Long, Integer> item2pop = new HashMap<Long, Integer>();
LongPrimitiveIterator iter = dataModel.getUserIDs();
while (iter.hasNext()) {
PreferenceArray prefs = dataModel.getPreferencesFromUser(iter
	.nextLong());
for (int i = 0; i < prefs.length(); i++) {
	Integer counter = item2pop.get(prefs.getItemID(i));
	if (counter == null) {
		item2pop.put(prefs.getItemID(i), 1);
	} else {
		item2pop.put(prefs.getItemID(i), counter + 1);
	}
      }
}
List<RecommendedItem> popitems = transfom2list(item2pop);
Collections.sort(itemidcounter, new RecommenderItemComparator()); 
return popitems
\end{lstlisting}


\begin{lstlisting}[caption={To transform SolrDocument to JSON Object we define a implicit definition of Writes[SolrDocment].}, label={lst:writes}]
implicit val solrdocWrites = new Writes[SolrDocument] {
  def writes(solrdoc: SolrDocument) = Json.obj(
     "title" -> solrdoc.getFieldValue("title").toString().filter { _ != '[' }.filter { ']' != _ },
     "id" -> solrdoc.getFieldValue("id").toString().toLong)
}  
\end{lstlisting}

\section{Apache Solr}
\label{sec:solr}

Apache Solr is a search engine that is optimized to search large volumes of text-centric data and return results sorted by relevance. It is built on Apache Lucene, an information retrieval library \cite{grainger}. Solr stores the documents in a flat structure and we can search for terms in the documents. It provides a HTTP API for all interactions, like quering, updating e.t.c.

The reason why we deploy a search engine in order to make recommendations is that Solr scores documents based on the presence of query terms in the document similar to a recommendations engine based on the presence of indicator.

Another reason why we deploy a search engine is that the application is read-dominant. The recommender will query the data far more often than it will create new documents or update the indicators. Solr is optimized to for executing queries as opposed to storing data.

\section{Apache Mahout}
\label{sec:mahout}

Apache mahout is a top-level Apache project that provide implementations of collaborative filtering algorithms among other machine learning techniques. The development began in 2008 \cite{Owen}. It provides server Apache Spark jobs in order to process large amount of data. We used it because it's Spark support and because it's well documented.
Mahout provides a large set of recommender algorithms. And it is easy to evaluate and compare several algorithms on a specific dataset.

\subsection{Data representation}
\label{sec:datarepresentation}

Apache Mahout uses it's own data structures to store and access preference data. Mahout provides it's own map implementation \verb|FastByIDMap|. Keys in \verb|FastByIDMap| are long primitives instead of java objects. In addition \verb|FastByIDMap| has no additional \verb|Map.Entry| object per entry. The avoidance of java object saves memory. Depending on the implementation of the JVM a java object allocates about 28 bytes of memory. \verb|FastByIDMap| consumes about 28 bytes per entry, compared to about 84 byte per entry for \verb|HashMap| from \verb|java.util|.

\section{Log-likelihood ratio}
\label{sec:llrratio}
The log-likelihood metric quantifies the propability of how
unlikely it is that two items interact with the same users is due to chance. The less likely, the more similar we are.
In order to compute the \gls{llr} ratio of the \gls{coocc} among item $A$ and item $B$ we have to count how many users interacted both with $A$ and $B$, how many users interacted with $A$ without interacting with $B$, how many users interacted with $B$ without interacting with $A$ and how many user interacted with none of them.

 \todo{llr contency table einfuegen und formel fuer llr}

\section{Infrastructur}
\label{sec:infrastructur}

\subsection{Webapplication}
\label{sec:web}

To start the Webapplication run:
\begin{verbatim}
sbt build 
\end{verbatim}
This assumes that the scala build tool (sbt) is installed.
\subsection{Apache Spark}
\label{sec:spark}
Apache Spark is a cluster computer framewor. It allows user programs to load data into a cluster's memory. It is well suited to machine learning algorithms \cite{Karau}.
\subsubsection{How to deploy Spark}
\label{sec:sparkdeploy}

To run \verb|spark-itemsimilarity| we have to deploy Apache Spark and and start it in standalone mode with the script \verb|start-master.sh|. Once startet the script will print a URL which we can pass the \verb|spark-itemsimilarity| job as ``master'' argument and the job can connect to the cluster. 

\section{Sample Input Data}
\label{sec:sampleinput}

\begin{lstlisting}[label=lst:sampledata]
itemid, userid, timestamp
1,101,980730861
1,102,980731380
1,103,980731926
2,101,980732037
2,103,980730408
2,104,980731766
3,101,980731282
3,102,980730769
3,103,980731208
4,102,980732235
4,103,980731417
5,101,980731745
5,102,980731621
5,103,980731417
5,104,980731208  
\end{lstlisting}

