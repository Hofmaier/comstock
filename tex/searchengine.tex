\subsection{Why can we use a search engine to produce \gls{topn}?}
\label{sec:relation}

The \gls{rec} uses a search engine to produce a \gls{topn}. We can deploy a search engine in order to provide recommendations because there are similarities between the computation of the \gls{topnt} and the retrieval of a ranked search result set.
 This section explains why the deployment of a search engine is suitable for the top-N recommendation task.

\subsubsection{Ranked retrieval}
A search engine enables user to search a collection of documents for specified keywords in a query. It returns a sorted set of documents that match the query. The result set is sorted by relevancy. The top documents are the most relevant to the query. This process is called \gls{rankedretrieval}. It does this by calculating a similarity score between each document and the query and then sorts the result by this score. The score indicates the strength of the match against the query. This is one of the main use cases where search engines shine compared to relational databases. There a row either matches a query or it does not. 

\subsubsection{Vector space model}
One way to calculate the similarity between a query $q$ and a document $d$ is to use the vector space model.
In the vector space model each document $d$ and the query $q$ are represented as vectors $\vec{v}(d)$ and $\vec{v}(q)$. The vector contains an element for each term. It maps every term $t$ of the collection to a tf-idf weight. tf-idf reflects how important a term is to a document in the collection (see \cite{Manning} for a detailed description). 
The similarity score between two items is equal to the dot product.
\begin{equation}
  \label{eq:score}
  \text{score}(d,q) = \vec{v}(d) \cdot \vec{v}(q)
\end{equation}
In order to create a ranked result set for a query $q$ the search engine computes $\text{score}(d,q)$ for all documents in the collection. We can form a matrix $C$ with the document vectors as rows. The process of scoring all document can be writen as matrix vector multiplication of $C$ and $q$. 

\begin{equation}
  \label{eq:ser}
  r = C q
\end{equation}
$r$ maps every document to a relevancy score.
This is similar to the computation of the \gls{topn} $r_u = M h_u$  described in section \ref{sec:problem}. The search engine returns documents that are similar to the query $q$. The recommender returns items that are similar to the items in the user's action history $h_u$. If we can map items to documents and the user's action history to a query we can use an existing search engine for the top-N recommendation task. This is desirable because search engines like Apache Solr are optimized for ranked retrival and they are able to process big data at scale. 

\subsubsection{How to map documents to items}

\begin{lstlisting}[caption={Item metadata and similar items are stored in Solr.},label={lst:solrdoc}]
{
    id: 1,
    title: Toy Story,
    tags:Pixar animation fantasy,
    likeindicator: 1688 1834 3893 4366 6281 33162,
    tagindicator: 10 33 41 54 55 59 66 67 72 73 80
    _version_: 1505056335358591000
}
\end{lstlisting}

The goal of our mapping is that the search engine finds the most similar items to the one in the user's action history.
The result of the search should contain a list of items. Hence we index items instead of documents.

A document in a search engine contains fields. A field contains a sequence of terms or metadata. 
Instead of finding document that contain the keywords of the query in a field we want to find items that are similar to the items in the user's recent action history. Therefore we replace the terms (words in free text) with item-id's that are similar to the item represented by the document. 

Figure \ref{lst:solrdoc} shows an example entry of a movie item formated in JSON. The indicator fields \verb|likeindicator| and \verb|tagindicator| contain movie ID's of similar movies. In addition to the indicators, the entry contains metadata about the item. These fields can be used to retrieve items by metadata, such as title or genre.

A search engine using the vector space model will use document vectors with tf-idf values to compute the score of a match. Tf-idf will mitigate popular items which is desirable but but it only accounts for the occurence of a term in a text. With the proposed mapping the term frequency of indicator ID's will be 1 or 0. We want to have some way to include the \gls{llr} ratios to boost items that have a high similarity to the item represented by the document. In order to include the similarity metric among items described in section \ref{sec:llr} we have to weight the tf-idf values in the document vector with the \gls{llr} ratios. This is the reason why we can't use a search engine out of the box without extending it's scoring function.
We have to replace the document vector with rows of the \gls{indicatorm} $M$. 

\subsubsection{Composing the query}
We store items with metadata and the corresponding similar items as a document in the search engine. To get a \gls{topn} we query the search engine with items from the user's action history $h$. The search engine will transform the item in the query to the corresponding document vector. Then it will find all items that have those recent history items as indicators. The more items the query $h$ and an item have in common the higher the similarity score. Hence we get ranked list with the most similar items on top. Table \ref{tbl:comparison} shows the mapping.

\begin{table}
\begin{center}
\begin{tabular}{lll}
 search engine & \gls{rec}\\ \hline
  document & item\\ 
 field & indicator \\
 term & indicator id    \\
 query & user's action history \\
\end{tabular}
\end{center}
\caption{We can map document, fields, term and query to the recommender equivalents. A search engine is similar to a recommender. We use the user'action history as query to find items in the collection that have those recent history items as indicators. }
\label{tbl:comparison}
\end{table}

Note that we can emulate different recommendation engines by using different queries.
\begin{itemize}
\item If the query is composed with the user's action history we use collaborative filtering with an itembased approach.
\item We can use a single item $i$ as query and get back all similar items. For instance, a top-N list of items retrieved with this query can be placed on a description page of item $i$ with the title ``Customers Who Liked This Item Also Liked''. This is a non-personalized approach.
\item We can use user profile information in the query. If the user profile contains information about the user's favorite movie genre or favourite regiseur and the search engine has index the corresponding metadata we retrieve a scored search result with respect to the fields genre and regiseur. This would be a content based approach.
\end{itemize}

\subsubsection{Implementation}
\label{sec:solrimpl}
We deploy the search engine Apache Solr in our demo webapplication. Simply put, Solr is a Web API for Apache Lucene, an open source information retrieval software library. 

In order to use the precomputed \gls{llr} ratios of the \glspl{coocc} we need a way to store them per term and per document. Lucene provides this functionality with the \emph{playloads} feature. Payloads enable an application to store an arbitrary byte array for every term during indexing \cite{McCandless}.

Unfortunatly the feature is only implemented in Lucence, not in Solr. But we can extend Solr to use the Java classes that deal with payloads.

First we need to define a new fieldtype in the schema.xml of our Solr configuration. Fields of this type contain terms with an added payload.
To index the similarity value we define a custom fieltype that uses Lucene's DelimitedPayloadTokenFilter to extract a payload from every term. 

\begin{lstlisting}[caption={Fieldtype definition for field with payload.}]
<fieldtype name="payloads" class="solr.TextField" >
 <analyzer>
  <tokenizer class="solr.WhitespaceTokenizerFactory"/>
   <filter class="DelimitedPayloadTokenFilterFactory" />
 </analyzer>
</fieldtype>
\end{lstlisting}
The default delimiter is the pipe symbol. With this fieldtype we can add payloads to terms in document be attaching a number to the term string.
\begin{verbatim}
23|0.9
\end{verbatim}
In this example \verb|23| is an item-id and \verb|0.9| is the similiarity value to the corresponding item.

After we stored the payload within the index we need a way to score a document match according the payload.
Solr uses per default the vector space model to compute the score between a document $d$ and a query $q$. It only considers terms $t$ that appear in the query $q$. The tf-idf weight is multiplied by a boosting factor for a term $t$ \cite{grainger}.  
\begin{equation}
\label{eq:solrsim}
\text{score}(q,d) = \sum_{t \in q} \text{tf-idf}(t,d) \cdot \text{boost}(t)
\end{equation}

In order to use the payload values in the ranked retrieval process we have to extend Solr with our own Query Component. 

We can add a custom Query Component by implementing a \verb|QParserPlugin|. The implementation just returns the \verb|PayloadTermQuery|. \verb|PayloadTermQuery| matches all documents containing the specifed term and than applies a scoring factor based on the payload that is stored with each term.

In addition we have to override the method \verb|scorePayload| of the \\ \verb|DefaultSimilarity| class. \verb|PayloadTermQuery| will call \verb|scorePayload| to determine the payload of an item and then it multiplies the td-idf weights with the corresponding payload. We have to tell Solr to use our custom Query Component and the new \verb|PayloadSimilarity| by adding the following entries to the solrconfig.xml.

\begin{lstlisting}
<similarity class="ch.hsr.solrpayload.PayloadSimilarityFactory">
<queryParser name="payloadparser" 
   class="ch.hsr.solrpayload.PayloadParser" />
\end{lstlisting}
  
Solr will load the implementations from it's \verb|lib| directory if we package them Java jar archive.


\begin{figure}
  \centering
\begin{tikzpicture}
  \begin{axis}[
    title=evaluation of LLR threshold,
    xlabel=threshold,
    ylabel=Precision/Recall,
]
\addplot table {thresholdp.dat};
\addplot table {thresholdr.dat};

  \end{axis}
\end{tikzpicture}
\caption{The precision and recall at 10 for the movie lens test data set is measured as a function of the similarity threshold. Items with a small similarity value do not contribute to the performance. The best performance is archieved by only adding items with a \gls{llr} ratio above 0.9.}
\label{fig:threshold}
\end{figure}

In our demo web application we only add items above a given \gls{llr} ratio threshold to the index of the search engine. If we add items with a small similarity to the indicator fields the precision and recall decline. The evaluation in figure \ref{fig:threshold}shows that the we archieve the best performance with a threshold of 0.9.

An alternative to the use of payloads is to ignore similarities values once we removed the item below threshold from the index. In that case the scoring of a document only depends on the tf-idf weight. Only add items above a threshold to the indicator fields. This approach is easier to deploy but it doesn't account for different similaries. 